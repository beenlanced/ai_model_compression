{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24b0cdf",
   "metadata": {},
   "source": [
    "# Layer-based Pruning Approach Using PyTorch\n",
    "\n",
    "The general work flow:\n",
    "\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Layer-based Pruning Workflow\" src=\"layer_based_prune_workflow.png\" width=\"450\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "[img source: pruning section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08ee4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc99b4f",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc519c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import torch.optim as optim\n",
    "import torch.utils.mobile_optimizer as mobile_optimizer\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b24b83",
   "metadata": {},
   "source": [
    "### Set Random Seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a164c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483c912",
   "metadata": {},
   "source": [
    "### Do an initial check if Compute Unified Device Architecture (CUDA) is available\n",
    "\n",
    "Checking if CUDA is available is a crucial step in applications, particularly in deep learning and high-performance computing, for several reasons: \n",
    "\n",
    "* Enabling GPU Acceleration:\n",
    "\n",
    "    CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and API that allows software to leverage the power of NVIDIA GPUs for general-purpose computing. Checking for its availability determines whether your program can offload computationally intensive tasks to the GPU, leading to significant speedups compared to CPU-only execution.\n",
    "\n",
    "* Conditional Code Execution:\n",
    "\n",
    "    By checking for CUDA availability, you can write code that dynamically adapts to the hardware environment. If a CUDA-enabled GPU is present, your program can utilize GPU-specific operations and data structures. If not, it can gracefully fall back to CPU implementations or inform the user about the lack of GPU support. This prevents errors and ensures your application can run on various systems.\n",
    "\n",
    "* Resource Management:\n",
    "\n",
    "    Knowing if CUDA is available allows you to manage resources effectively. If a GPU is present, you can allocate memory on the device and perform computations there. If not, you avoid attempting to access non-existent GPU resources, which would lead to errors.\n",
    "\n",
    "* Error Prevention and Debugging:\n",
    "\n",
    "    Explicitly checking for CUDA availability helps in identifying and preventing issues related to missing or improperly configured CUDA installations or incompatible GPU drivers. If the check fails, it provides an immediate indication that GPU acceleration is not possible, guiding troubleshooting efforts.\n",
    "\n",
    "* Optimized Performance:\n",
    "\n",
    "    Many deep learning frameworks and libraries are designed to leverage CUDA for optimal performance. Verifying CUDA availability ensures that these frameworks can utilize the intended hardware acceleration, leading to faster training times and inference speeds for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac71e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for training: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f\"Using device for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7404c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Building the Model: Model Definition\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "  <img alt=\"CNN model\" src=\"layer_prune_cnn.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: pruning section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e409459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "   def __init__(self, in_channels=1, conv1_channels=32, conv2_channels=64, fc1_units=128, fc2_units=10):\n",
    "       super(SimpleCNN, self).__init__()\n",
    "       self.conv1 = nn.Conv2d(in_channels, conv1_channels, kernel_size=3, padding=1)\n",
    "       self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, kernel_size=3, padding=1)\n",
    "       self.pool = nn.MaxPool2d(2, 2)\n",
    "       self.fc1 = nn.Linear(conv2_channels * 7 * 7, fc1_units)\n",
    "       self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "       self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.pool(self.relu(self.conv1(x)))\n",
    "       x = self.pool(self.relu(self.conv2(x)))\n",
    "       x = x.view(-1, self.conv2.out_channels * 7 * 7)\n",
    "       x = self.relu(self.fc1(x))\n",
    "       x = self.fc2(x)\n",
    "       return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5d312",
   "metadata": {},
   "source": [
    "### Loading the Modified National Institute of Standards and Technology (MNIST) Dataset\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a widely used dataset in the field of machine learning, particularly for image recognition and classification tasks.\n",
    "\n",
    "#### Key characteristics of the MNIST dataset:\n",
    "\n",
    "* **Handwritten Digits:**\n",
    "    - It consists of a large collection of grayscale images of handwritten digits (0-9).\n",
    "\n",
    "* **Image Dimensions:**\n",
    "    - Each image is a 28x28 pixel grayscale image.\n",
    "\n",
    "* **Dataset Size:**\n",
    "    - It comprises a training set of 60,000 examples and a test set of 10,000 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0090a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_data():\n",
    "   transform = transforms.Compose([\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize((0.1307,), (0.3081,))\n",
    "   ])\n",
    "\n",
    "\n",
    "   trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "   trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "   testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "   testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "   return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cc168",
   "metadata": {},
   "source": [
    "## Utils for Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b1313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy\n",
    "def evaluate_model(model, testloader):\n",
    "   model.to(device)\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = 0\n",
    "   with torch.no_grad():\n",
    "       for data in testloader:\n",
    "           images, labels = data[0].to(device), data[1].to(device)\n",
    "           outputs = model(images)\n",
    "           _, predicted = torch.max(outputs.data, 1)\n",
    "           total += labels.size(0)\n",
    "           correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "   accuracy = 100 * correct / total\n",
    "   return accuracy\n",
    "\n",
    "\n",
    "# Measure inference time more accurately\n",
    "def measure_inference_time(model, testloader, num_runs=3):\n",
    "   model.to(device)\n",
    "   model.eval()\n",
    "\n",
    "\n",
    "   # Warm-up run\n",
    "   with torch.no_grad():\n",
    "       for data in testloader:\n",
    "           images = data[0].to(device)\n",
    "           _ = model(images)\n",
    "\n",
    "\n",
    "   # Measure inference time\n",
    "   torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "   total_time = 0\n",
    "\n",
    "\n",
    "   for _ in range(num_runs):\n",
    "       start_time = time.time()\n",
    "       with torch.no_grad():\n",
    "           for data in testloader:\n",
    "               images = data[0].to(device)\n",
    "               _ = model(images)\n",
    "       torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "       end_time = time.time()\n",
    "       total_time += (end_time - start_time)\n",
    "\n",
    "\n",
    "   return total_time / num_runs\n",
    "\n",
    "\n",
    "# Get model size in MB (more accurately)\n",
    "def get_model_size(model, filename=\"temp_model.pth\"):\n",
    "   # Save model in different formats to compare\n",
    "   torch.save(model.state_dict(), filename)\n",
    "   state_dict_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "\n",
    "\n",
    "   # Save as TorchScript for better compression of sparse models\n",
    "   script_model = torch.jit.script(model.cpu())\n",
    "   optimized_script_model = mobile_optimizer.optimize_for_mobile(script_model)\n",
    "   script_filename = filename.replace('.pth', '.pt')\n",
    "   optimized_script_model.save(script_filename)\n",
    "   script_size = os.path.getsize(script_filename) / (1024 * 1024)\n",
    "\n",
    "\n",
    "   # Clean up files\n",
    "   if os.path.exists(filename):\n",
    "       os.remove(filename)\n",
    "   if os.path.exists(script_filename):\n",
    "       os.remove(script_filename)\n",
    "\n",
    "\n",
    "   return state_dict_size, script_size\n",
    "\n",
    "\n",
    "# Count non-zero parameters\n",
    "def count_parameters(model):\n",
    "   total_params = 0\n",
    "   nonzero_params = 0\n",
    "   for param in model.parameters():\n",
    "       total_params += param.numel()\n",
    "       nonzero_params += torch.sum(param != 0).item()\n",
    "   return total_params, nonzero_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ebfa8",
   "metadata": {},
   "source": [
    "## Training the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58b012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, trainloader, epochs=3):\n",
    "   model.to(device)\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "   for epoch in range(epochs):\n",
    "       running_loss = 0.0\n",
    "       for i, data in enumerate(trainloader, 0):\n",
    "           inputs, labels = data[0].to(device), data[1].to(device)\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs)\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           running_loss += loss.item()\n",
    "\n",
    "\n",
    "           if i % 100 == 99:\n",
    "               print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "               running_loss = 0.0\n",
    "\n",
    "\n",
    "   print('Finished Training')\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c196989",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Unstructured Pruning\n",
    "\n",
    "___\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Unstructured model\" src=\"layer_prune_unstructured.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: pruning section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed0f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unstructured pruning with parameter removal\n",
    "def apply_unstructured_pruning(model, prune_amount=0.5):\n",
    "   print(\"Applying unstructured pruning...\")\n",
    "   pruned_model = copy.deepcopy(model)\n",
    "   pruned_model.cpu()  # Move to CPU for pruning operations\n",
    "\n",
    "\n",
    "   # Apply pruning to all conv and linear layers\n",
    "   for name, module in pruned_model.named_modules():\n",
    "       if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "           prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "           # Make pruning permanent (removes the mask)\n",
    "           prune.remove(module, 'weight')\n",
    "\n",
    "\n",
    "   # For actual model size reduction in a real system, we would need\n",
    "   # to convert this to a sparse format or create a new model with fewer parameters\n",
    "   return pruned_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3a6bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Structured Pruning\n",
    "\n",
    "___\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Structured model\" src=\"layer_prune_structured.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: pruning section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3fbcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d382a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement actual structured pruning (removing channels)\n",
    "def apply_structured_pruning(model, prune_amount=0.3):\n",
    "   print(\"Applying structured pruning...\")\n",
    "   original_model = model.cpu()  # Move to CPU for pruning operations\n",
    "\n",
    "\n",
    "   # Get the original architecture parameters\n",
    "   original_conv1_out = original_model.conv1.out_channels\n",
    "   original_conv2_out = original_model.conv2.out_channels\n",
    "   original_fc1_out = original_model.fc1.out_features\n",
    "\n",
    "\n",
    "   # Calculate new dimensions after pruning\n",
    "   new_conv1_out = int(original_conv1_out * (1 - prune_amount))\n",
    "   new_conv2_out = int(original_conv2_out * (1 - prune_amount))\n",
    "   new_fc1_out = int(original_fc1_out * (1 - prune_amount))\n",
    "\n",
    "\n",
    "   # Create new model with reduced dimensions\n",
    "   pruned_model = SimpleCNN(\n",
    "       in_channels=1,\n",
    "       conv1_channels=new_conv1_out,\n",
    "       conv2_channels=new_conv2_out,\n",
    "       fc1_units=new_fc1_out,\n",
    "       fc2_units=10  # Output dimension remains the same\n",
    "   )\n",
    "\n",
    "\n",
    "   # For each layer, determine which channels to keep based on L2 norm\n",
    "   # Conv1 layer\n",
    "   conv1_weight = original_model.conv1.weight.data\n",
    "   conv1_channel_norms = torch.norm(conv1_weight, p=2, dim=[1, 2, 3])\n",
    "   _, conv1_indices = torch.topk(conv1_channel_norms, new_conv1_out)\n",
    "   conv1_indices = sorted(conv1_indices.tolist())\n",
    "\n",
    "\n",
    "   # Copy weights for kept channels in conv1\n",
    "   pruned_model.conv1.weight.data = conv1_weight[conv1_indices]\n",
    "   pruned_model.conv1.bias.data = original_model.conv1.bias.data[conv1_indices]\n",
    "\n",
    "\n",
    "   # Conv2 layer (need to adjust input channels to match Conv1 output)\n",
    "   conv2_weight = original_model.conv2.weight.data\n",
    "   conv2_channel_norms = torch.norm(conv2_weight, p=2, dim=[1, 2, 3])\n",
    "   _, conv2_indices = torch.topk(conv2_channel_norms, new_conv2_out)\n",
    "   conv2_indices = sorted(conv2_indices.tolist())\n",
    "\n",
    "\n",
    "   # Create a new weight tensor for conv2 with adjusted dimensions\n",
    "   pruned_model.conv2.weight.data = torch.zeros(\n",
    "       new_conv2_out, new_conv1_out,\n",
    "       conv2_weight.size(2), conv2_weight.size(3)\n",
    "   )\n",
    "\n",
    "\n",
    "   # Copy weights for kept channels, adjusting for input channels\n",
    "   for i, out_idx in enumerate(conv2_indices):\n",
    "       for j, in_idx in enumerate(conv1_indices):\n",
    "           pruned_model.conv2.weight.data[i, j] = conv2_weight[out_idx, in_idx]\n",
    "\n",
    "\n",
    "   pruned_model.conv2.bias.data = original_model.conv2.bias.data[conv2_indices]\n",
    "\n",
    "\n",
    "   # FC1 layer (need to adjust input to match Conv2 output)\n",
    "   fc1_weight = original_model.fc1.weight.data\n",
    "   fc1_output_norms = torch.norm(fc1_weight, p=2, dim=1)\n",
    "   _, fc1_indices = torch.topk(fc1_output_norms, new_fc1_out)\n",
    "   fc1_indices = sorted(fc1_indices.tolist())\n",
    "\n",
    "\n",
    "   # Create a new weight tensor with adjusted dimensions\n",
    "   pruned_model.fc1.weight.data = torch.zeros(\n",
    "       new_fc1_out, new_conv2_out * 7 * 7\n",
    "   )\n",
    "\n",
    "\n",
    "   # This is a bit tricky - we need to reshape both matrices to account for\n",
    "   # the changed conv2 output channels\n",
    "   reshaped_old = fc1_weight.view(original_fc1_out, original_conv2_out, 7, 7)\n",
    "   reshaped_new = pruned_model.fc1.weight.data.view(new_fc1_out, new_conv2_out, 7, 7)\n",
    "\n",
    "\n",
    "   for i, out_idx in enumerate(fc1_indices):\n",
    "       for j, in_idx in enumerate(conv2_indices):\n",
    "           reshaped_new[i, j] = reshaped_old[out_idx, in_idx]\n",
    "\n",
    "\n",
    "   pruned_model.fc1.bias.data = original_model.fc1.bias.data[fc1_indices]\n",
    "\n",
    "\n",
    "   # FC2 layer\n",
    "   fc2_weight = original_model.fc2.weight.data\n",
    "   # Create a new weight tensor for FC2 with adjusted dimensions\n",
    "   pruned_model.fc2.weight.data = torch.zeros(10, new_fc1_out)\n",
    "\n",
    "\n",
    "   # Copy weights for kept FC1 output units\n",
    "   for j, in_idx in enumerate(fc1_indices):\n",
    "       pruned_model.fc2.weight.data[:, j] = fc2_weight[:, in_idx]\n",
    "\n",
    "\n",
    "   pruned_model.fc2.bias.data = original_model.fc2.bias.data\n",
    "\n",
    "\n",
    "   return pruned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb322d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine-Tuning \n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Fine-Tuning the Model\" src=\"layer_prune_fine_tuning.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: pruning section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3c4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the pruned model\n",
    "def fine_tune_model(model, trainloader, epochs=2):\n",
    "   model.to(device)\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "   print(\"Fine-tuning the pruned model...\")\n",
    "   for epoch in range(epochs):\n",
    "       running_loss = 0.0\n",
    "       for i, data in enumerate(trainloader, 0):\n",
    "           inputs, labels = data[0].to(device), data[1].to(device)\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs)\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           running_loss += loss.item()\n",
    "\n",
    "\n",
    "           if i % 100 == 99:\n",
    "               print(f'Fine-tuning Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "               running_loss = 0.0\n",
    "\n",
    "\n",
    "   print('Finished Fine-tuning')\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b69f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Main Function \n",
    "\n",
    "---\n",
    "\n",
    "Run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15b05a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the experiment\n",
    "def main():\n",
    "\n",
    "   # Load data\n",
    "   trainloader, testloader = load_data()\n",
    "\n",
    "\n",
    "   # Initialize and train the model\n",
    "   print(\"Training original model...\")\n",
    "   original_model = SimpleCNN()\n",
    "   original_model = train_model(original_model, trainloader)\n",
    "\n",
    "\n",
    "   # Evaluate original model\n",
    "   print(\"\\nEvaluating original model...\")\n",
    "   original_accuracy = evaluate_model(original_model, testloader)\n",
    "   original_inference_time = measure_inference_time(original_model, testloader)\n",
    "   original_state_dict_size, original_script_size = get_model_size(original_model)\n",
    "   total_params, nonzero_params = count_parameters(original_model)\n",
    "\n",
    "\n",
    "   print(\"\\n--- Original Model Metrics ---\")\n",
    "   print(f\"Accuracy: {original_accuracy:.2f}%\")\n",
    "   print(f\"Inference Time: {original_inference_time:.4f} seconds\")\n",
    "   print(f\"Model Size (state_dict): {original_state_dict_size:.2f} MB\")\n",
    "   print(f\"Model Size (TorchScript): {original_script_size:.2f} MB\")\n",
    "   print(f\"Total Parameters: {total_params}\")\n",
    "   print(f\"Non-zero Parameters: {nonzero_params} ({nonzero_params/total_params*100:.2f}%)\")\n",
    "\n",
    "\n",
    "   # Clear memory\n",
    "   gc.collect()\n",
    "   torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "\n",
    "   # Apply unstructured pruning\n",
    "   unstructured_model = apply_unstructured_pruning(original_model)\n",
    "\n",
    "\n",
    "   # Evaluate unstructured pruned model\n",
    "   print(\"\\nEvaluating unstructured pruned model...\")\n",
    "   unstructured_accuracy = evaluate_model(unstructured_model, testloader)\n",
    "   unstructured_inference_time = measure_inference_time(unstructured_model, testloader)\n",
    "   unstructured_state_dict_size, unstructured_script_size = get_model_size(unstructured_model)\n",
    "   total_params_u, nonzero_params_u = count_parameters(unstructured_model)\n",
    "\n",
    "\n",
    "   print(\"\\n--- Unstructured Pruned Model Metrics ---\")\n",
    "   print(f\"Accuracy: {unstructured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Time: {unstructured_inference_time:.4f} seconds\")\n",
    "   print(f\"Model Size (state_dict): {unstructured_state_dict_size:.2f} MB\")\n",
    "   print(f\"Model Size (TorchScript): {unstructured_script_size:.2f} MB\")\n",
    "   print(f\"Total Parameters: {total_params_u}\")\n",
    "   print(f\"Non-zero Parameters: {nonzero_params_u} ({nonzero_params_u/total_params_u*100:.2f}%)\")\n",
    "   print(f\"Accuracy Drop: {original_accuracy - unstructured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Speedup: {original_inference_time / unstructured_inference_time:.2f}x\")\n",
    "   print(f\"Size Reduction (state_dict): {(1 - unstructured_state_dict_size / original_state_dict_size) * 100:.2f}%\")\n",
    "   print(f\"Size Reduction (TorchScript): {(1 - unstructured_script_size / original_script_size) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "   # Clear memory\n",
    "   gc.collect()\n",
    "   torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "\n",
    "   # Apply structured pruning\n",
    "   structured_model = apply_structured_pruning(original_model)\n",
    "\n",
    "\n",
    "   # Evaluate structured pruned model\n",
    "   print(\"\\nEvaluating structured pruned model...\")\n",
    "   structured_accuracy = evaluate_model(structured_model, testloader)\n",
    "   structured_inference_time = measure_inference_time(structured_model, testloader)\n",
    "   structured_state_dict_size, structured_script_size = get_model_size(structured_model)\n",
    "   total_params_s, nonzero_params_s = count_parameters(structured_model)\n",
    "\n",
    "\n",
    "   print(\"\\n--- Structured Pruned Model Metrics ---\")\n",
    "   print(f\"Accuracy: {structured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Time: {structured_inference_time:.4f} seconds\")\n",
    "   print(f\"Model Size (state_dict): {structured_state_dict_size:.2f} MB\")\n",
    "   print(f\"Model Size (TorchScript): {structured_script_size:.2f} MB\")\n",
    "   print(f\"Total Parameters: {total_params_s}\")\n",
    "   print(f\"Non-zero Parameters: {nonzero_params_s} ({nonzero_params_s/total_params_s*100:.2f}%)\")\n",
    "   print(f\"Accuracy Drop: {original_accuracy - structured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Speedup: {original_inference_time / structured_inference_time:.2f}x\")\n",
    "   print(f\"Size Reduction (state_dict): {(1 - structured_state_dict_size / original_state_dict_size) * 100:.2f}%\")\n",
    "   print(f\"Size Reduction (TorchScript): {(1 - structured_script_size / original_script_size) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "   # Fine-tune unstructured pruned model\n",
    "   fine_tuned_unstructured = fine_tune_model(unstructured_model, trainloader)\n",
    "\n",
    "\n",
    "   # Evaluate fine-tuned unstructured pruned model\n",
    "   print(\"\\nEvaluating fine-tuned unstructured pruned model...\")\n",
    "   fine_tuned_unstructured_accuracy = evaluate_model(fine_tuned_unstructured, testloader)\n",
    "   fine_tuned_unstructured_inference = measure_inference_time(fine_tuned_unstructured, testloader)\n",
    "\n",
    "\n",
    "   print(\"\\n--- Fine-tuned Unstructured Pruned Model Metrics ---\")\n",
    "   print(f\"Accuracy: {fine_tuned_unstructured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Time: {fine_tuned_unstructured_inference:.4f} seconds\")\n",
    "   print(f\"Accuracy Recovery: {fine_tuned_unstructured_accuracy - unstructured_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "   # Fine-tune structured pruned model\n",
    "   fine_tuned_structured = fine_tune_model(structured_model, trainloader)\n",
    "\n",
    "\n",
    "   # Evaluate fine-tuned structured pruned model\n",
    "   print(\"\\nEvaluating fine-tuned structured pruned model...\")\n",
    "   fine_tuned_structured_accuracy = evaluate_model(fine_tuned_structured, testloader)\n",
    "   fine_tuned_structured_inference = measure_inference_time(fine_tuned_structured, testloader)\n",
    "\n",
    "\n",
    "   print(\"\\n--- Fine-tuned Structured Pruned Model Metrics ---\")\n",
    "   print(f\"Accuracy: {fine_tuned_structured_accuracy:.2f}%\")\n",
    "   print(f\"Inference Time: {fine_tuned_structured_inference:.4f} seconds\")\n",
    "   print(f\"Accuracy Recovery: {fine_tuned_structured_accuracy - structured_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "   # Summary comparison\n",
    "   print(\"\\n--- Summary ---\")\n",
    "   print(\"Model              | Accuracy | Inference Time | Size (MB) | Non-zero/Total Params\")\n",
    "   print(\"--------------------|----------|----------------|-----------|--------------------\")\n",
    "   print(f\"Original            | {original_accuracy:.2f}%   | {original_inference_time:.4f}s        | {original_script_size:.2f}    | {nonzero_params}/{total_params} ({nonzero_params/total_params*100:.1f}%)\")\n",
    "   print(f\"Unstructured Pruned | {unstructured_accuracy:.2f}%   | {unstructured_inference_time:.4f}s        | {unstructured_script_size:.2f}    | {nonzero_params_u}/{total_params_u} ({nonzero_params_u/total_params_u*100:.1f}%)\")\n",
    "   print(f\"+ Fine-tuned        | {fine_tuned_unstructured_accuracy:.2f}%   | {fine_tuned_unstructured_inference:.4f}s        | {unstructured_script_size:.2f}    | {nonzero_params_u}/{total_params_u} ({nonzero_params_u/total_params_u*100:.1f}%)\")\n",
    "   print(f\"Structured Pruned   | {structured_accuracy:.2f}%   | {structured_inference_time:.4f}s        | {structured_script_size:.2f}    | {nonzero_params_s}/{total_params_s} ({nonzero_params_s/total_params_s*100:.1f}%)\")\n",
    "   print(f\"+ Fine-tuned        | {fine_tuned_structured_accuracy:.2f}%   | {fine_tuned_structured_inference:.4f}s        | {structured_script_size:.2f}    | {nonzero_params_s}/{total_params_s} ({nonzero_params_s/total_params_s*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e50f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training original model...\n",
      "Epoch 1, Batch 100, Loss: 0.570\n",
      "Epoch 1, Batch 200, Loss: 0.163\n",
      "Epoch 1, Batch 300, Loss: 0.099\n",
      "Epoch 1, Batch 400, Loss: 0.085\n",
      "Epoch 1, Batch 500, Loss: 0.088\n",
      "Epoch 1, Batch 600, Loss: 0.064\n",
      "Epoch 1, Batch 700, Loss: 0.066\n",
      "Epoch 1, Batch 800, Loss: 0.064\n",
      "Epoch 1, Batch 900, Loss: 0.048\n",
      "Epoch 2, Batch 100, Loss: 0.048\n",
      "Epoch 2, Batch 200, Loss: 0.040\n",
      "Epoch 2, Batch 300, Loss: 0.053\n",
      "Epoch 2, Batch 400, Loss: 0.036\n",
      "Epoch 2, Batch 500, Loss: 0.044\n",
      "Epoch 2, Batch 600, Loss: 0.040\n",
      "Epoch 2, Batch 700, Loss: 0.032\n",
      "Epoch 2, Batch 800, Loss: 0.037\n",
      "Epoch 2, Batch 900, Loss: 0.037\n",
      "Epoch 3, Batch 100, Loss: 0.032\n",
      "Epoch 3, Batch 200, Loss: 0.034\n",
      "Epoch 3, Batch 300, Loss: 0.029\n",
      "Epoch 3, Batch 400, Loss: 0.032\n",
      "Epoch 3, Batch 500, Loss: 0.022\n",
      "Epoch 3, Batch 600, Loss: 0.032\n",
      "Epoch 3, Batch 700, Loss: 0.020\n",
      "Epoch 3, Batch 800, Loss: 0.025\n",
      "Epoch 3, Batch 900, Loss: 0.026\n",
      "Finished Training\n",
      "\n",
      "Evaluating original model...\n",
      "\n",
      "--- Original Model Metrics ---\n",
      "Accuracy: 99.08%\n",
      "Inference Time: 1.9307 seconds\n",
      "Model Size (state_dict): 1.61 MB\n",
      "Model Size (TorchScript): 1.61 MB\n",
      "Total Parameters: 421642\n",
      "Non-zero Parameters: 421642 (100.00%)\n",
      "Applying unstructured pruning...\n",
      "\n",
      "Evaluating unstructured pruned model...\n",
      "\n",
      "--- Unstructured Pruned Model Metrics ---\n",
      "Accuracy: 98.19%\n",
      "Inference Time: 1.8993 seconds\n",
      "Model Size (state_dict): 1.61 MB\n",
      "Model Size (TorchScript): 1.61 MB\n",
      "Total Parameters: 421642\n",
      "Non-zero Parameters: 210938 (50.03%)\n",
      "Accuracy Drop: 0.89%\n",
      "Inference Speedup: 1.02x\n",
      "Size Reduction (state_dict): 0.00%\n",
      "Size Reduction (TorchScript): 0.00%\n",
      "Applying structured pruning...\n",
      "\n",
      "Evaluating structured pruned model...\n",
      "\n",
      "--- Structured Pruned Model Metrics ---\n",
      "Accuracy: 98.63%\n",
      "Inference Time: 1.6492 seconds\n",
      "Model Size (state_dict): 0.77 MB\n",
      "Model Size (TorchScript): 0.78 MB\n",
      "Total Parameters: 201849\n",
      "Non-zero Parameters: 201849 (100.00%)\n",
      "Accuracy Drop: 0.45%\n",
      "Inference Speedup: 1.17x\n",
      "Size Reduction (state_dict): 52.03%\n",
      "Size Reduction (TorchScript): 51.96%\n",
      "Fine-tuning the pruned model...\n",
      "Fine-tuning Epoch 1, Batch 100, Loss: 0.019\n",
      "Fine-tuning Epoch 1, Batch 200, Loss: 0.016\n",
      "Fine-tuning Epoch 1, Batch 300, Loss: 0.017\n",
      "Fine-tuning Epoch 1, Batch 400, Loss: 0.015\n",
      "Fine-tuning Epoch 1, Batch 500, Loss: 0.014\n",
      "Fine-tuning Epoch 1, Batch 600, Loss: 0.016\n",
      "Fine-tuning Epoch 1, Batch 700, Loss: 0.018\n",
      "Fine-tuning Epoch 1, Batch 800, Loss: 0.017\n",
      "Fine-tuning Epoch 1, Batch 900, Loss: 0.017\n",
      "Fine-tuning Epoch 2, Batch 100, Loss: 0.010\n",
      "Fine-tuning Epoch 2, Batch 200, Loss: 0.007\n",
      "Fine-tuning Epoch 2, Batch 300, Loss: 0.008\n",
      "Fine-tuning Epoch 2, Batch 400, Loss: 0.010\n",
      "Fine-tuning Epoch 2, Batch 500, Loss: 0.008\n",
      "Fine-tuning Epoch 2, Batch 600, Loss: 0.009\n",
      "Fine-tuning Epoch 2, Batch 700, Loss: 0.011\n",
      "Fine-tuning Epoch 2, Batch 800, Loss: 0.012\n",
      "Fine-tuning Epoch 2, Batch 900, Loss: 0.014\n",
      "Finished Fine-tuning\n",
      "\n",
      "Evaluating fine-tuned unstructured pruned model...\n",
      "\n",
      "--- Fine-tuned Unstructured Pruned Model Metrics ---\n",
      "Accuracy: 99.17%\n",
      "Inference Time: 1.8972 seconds\n",
      "Accuracy Recovery: 0.98%\n",
      "Fine-tuning the pruned model...\n",
      "Fine-tuning Epoch 1, Batch 100, Loss: 0.015\n",
      "Fine-tuning Epoch 1, Batch 200, Loss: 0.017\n",
      "Fine-tuning Epoch 1, Batch 300, Loss: 0.018\n",
      "Fine-tuning Epoch 1, Batch 400, Loss: 0.022\n",
      "Fine-tuning Epoch 1, Batch 500, Loss: 0.012\n",
      "Fine-tuning Epoch 1, Batch 600, Loss: 0.016\n",
      "Fine-tuning Epoch 1, Batch 700, Loss: 0.016\n",
      "Fine-tuning Epoch 1, Batch 800, Loss: 0.014\n",
      "Fine-tuning Epoch 1, Batch 900, Loss: 0.013\n",
      "Fine-tuning Epoch 2, Batch 100, Loss: 0.010\n",
      "Fine-tuning Epoch 2, Batch 200, Loss: 0.008\n",
      "Fine-tuning Epoch 2, Batch 300, Loss: 0.012\n",
      "Fine-tuning Epoch 2, Batch 400, Loss: 0.012\n",
      "Fine-tuning Epoch 2, Batch 500, Loss: 0.010\n",
      "Fine-tuning Epoch 2, Batch 600, Loss: 0.010\n",
      "Fine-tuning Epoch 2, Batch 700, Loss: 0.008\n",
      "Fine-tuning Epoch 2, Batch 800, Loss: 0.011\n",
      "Fine-tuning Epoch 2, Batch 900, Loss: 0.013\n",
      "Finished Fine-tuning\n",
      "\n",
      "Evaluating fine-tuned structured pruned model...\n",
      "\n",
      "--- Fine-tuned Structured Pruned Model Metrics ---\n",
      "Accuracy: 99.16%\n",
      "Inference Time: 1.6424 seconds\n",
      "Accuracy Recovery: 0.53%\n",
      "\n",
      "--- Summary ---\n",
      "Model              | Accuracy | Inference Time | Size (MB) | Non-zero/Total Params\n",
      "--------------------|----------|----------------|-----------|--------------------\n",
      "Original            | 99.08%   | 1.9307s        | 1.61    | 421642/421642 (100.0%)\n",
      "Unstructured Pruned | 98.19%   | 1.8993s        | 1.61    | 210938/421642 (50.0%)\n",
      "+ Fine-tuned        | 99.17%   | 1.8972s        | 1.61    | 210938/421642 (50.0%)\n",
      "Structured Pruned   | 98.63%   | 1.6492s        | 0.78    | 201849/201849 (100.0%)\n",
      "+ Fine-tuned        | 99.16%   | 1.6424s        | 0.78    | 201849/201849 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
