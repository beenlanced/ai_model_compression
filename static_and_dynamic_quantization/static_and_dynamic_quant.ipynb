{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27368724",
   "metadata": {},
   "source": [
    "# Dynamic, Static, and Quantitative-Aware Training Quantization Methods \n",
    "\n",
    "This notebook shows how to perform Dynamic, Static, and Quantitative-Aware Training Quantization.\n",
    "\n",
    "The notebook and materials are part of the LinkedIn Learning Course: [Ai Model Compression Techniques: Building Cheaper, Faster, and Greener AI:](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d056f4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a71a16",
   "metadata": {},
   "source": [
    "## Load Modules\n",
    "\n",
    "Some helpful comments on the modules:\n",
    "\n",
    "* `import torch` # torch will allow us to create tensors.\n",
    "\n",
    "* `import torch.nn as nn` # torch.nn allows us to create a neural network.\n",
    "\n",
    "* `torch.nn.functional` # nn.functional give us access to the activation and loss functions.\n",
    "\n",
    "* `import torch.optim as optim`  # optim contains many optimizers. This time I am using Adam\n",
    "\n",
    "* `from torch.utils.data import DataLoader, Subset` # needed for training data\n",
    "\n",
    "* `import torch.quantization` # provides tools and functionalities for quantizing deep learning models. \n",
    "\n",
    "* `import torch.utils.data import DataLoader, Subset` \n",
    "    * **DataLoader** loads data efficiently in batches\n",
    "    * **Subset** allows for the creation of an exisitng PyTorch Dataset. It takes an original Dataset and a list of indices as input, effectively creating a new \"dataset\" that only contains the samples corresponding to those specified indices. \n",
    "    \n",
    "* `from torchvision import datasets, transforms` \n",
    "    * **datasets**: This module provides access to a collection of popular datasets commonly used in computer vision for developing and testing machine learning models. It is where we get the CIFAR-10 dataset for this notebooks\n",
    "    * **transforms**: This module offers common image transformations and data augmentation techniques that are crucial for training and evaluating computer vision models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c565c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81f2b1",
   "metadata": {},
   "source": [
    "### Set Random Seeds for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe956da",
   "metadata": {},
   "source": [
    "### Do an initial check if Compute Unified Device Architecture (CUDA) is available\n",
    "\n",
    "Checking if CUDA is available is a crucial step in applications, particularly in deep learning and high-performance computing, for several reasons: \n",
    "\n",
    "* Enabling GPU Acceleration:\n",
    "\n",
    "    CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and API that allows software to leverage the power of NVIDIA GPUs for general-purpose computing. Checking for its availability determines whether your program can offload computationally intensive tasks to the GPU, leading to significant speedups compared to CPU-only execution.\n",
    "\n",
    "* Conditional Code Execution:\n",
    "\n",
    "    By checking for CUDA availability, you can write code that dynamically adapts to the hardware environment. If a CUDA-enabled GPU is present, your program can utilize GPU-specific operations and data structures. If not, it can gracefully fall back to CPU implementations or inform the user about the lack of GPU support. This prevents errors and ensures your application can run on various systems.\n",
    "\n",
    "* Resource Management:\n",
    "\n",
    "    Knowing if CUDA is available allows you to manage resources effectively. If a GPU is present, you can allocate memory on the device and perform computations there. If not, you avoid attempting to access non-existent GPU resources, which would lead to errors.\n",
    "\n",
    "\n",
    "* Error Prevention and Debugging:\n",
    "\n",
    "    Explicitly checking for CUDA availability helps in identifying and preventing issues related to missing or improperly configured CUDA installations or incompatible GPU drivers. If the check fails, it provides an immediate indication that GPU acceleration is not possible, guiding troubleshooting efforts.\n",
    "\n",
    "* Optimized Performance:\n",
    "\n",
    "    Many deep learning frameworks and libraries are designed to leverage CUDA for optimal performance. Verifying CUDA availability ensures that these frameworks can utilize the intended hardware acceleration, leading to faster training times and inference speeds for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "train_device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f\"Using device for training: {train_device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c6da0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Building the Model: Model Definition\n",
    "\n",
    "---\n",
    "\n",
    "I will be building a Convolutional Neueral Netwwork (CNN) for the CIFAR-10 Dataset.\n",
    "\n",
    "#### Why CNN?\n",
    "\n",
    "CNNs are preferred for CIFAR-10 image classification because they excel at identifying spatial hierarchies and patterns, which are crucial for recognizing objects within images. Their convolutional layers effectively extract features like edges, shapes, and textures, outperforming traditional Artificial Neural Networks (ANNs) for this task. \n",
    "\n",
    "The CIFAR-10 dataset consists of small, color images (32x32 pixels) with ten object categories. CNNs are well-suited to the spatial characteristics of these images and can effectively learn to classify them.\n",
    "\n",
    "CNN has a form similar to the figure below.\n",
    "\n",
    "<p>\n",
    "  <img alt=\"CNN for CIFAR-10\" src=\"cnn.png\" width=\"500\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "[img source](https://towardsdatascience.com/deep-learning-with-cifar-10-image-classification-64ab92110d79/)\n",
    "\n",
    "\n",
    "### Understanding the Key Components of the CNN Class Below\n",
    "\n",
    "* **class CIFAR10CNN(nn.Module):**\n",
    "\n",
    "    - This line declares a Python class **CIFAR10CNN** that inherits from nn.Module. Inheriting from nn.Module is fundamental in PyTorch for building neural networks, as it provides core functionalities like tracking parameters, moving models to different devices, and managing submodules.\n",
    "\n",
    "* **def __init__(self)::**\n",
    "\n",
    "    - This method is the constructor method where the layers of the neural network are defined and initialized.\n",
    "\n",
    "        - **super(CIFAR10CNN, self).__init__():** This calls the constructor of the parent class nn.Module, which is crucial for proper initialization of the PyTorch module.\n",
    "\n",
    "        - **self.features = nn.Sequential(...):** This defines the \"feature extraction\" part of the CNN using nn.Sequential. nn.Sequential allows chaining multiple layers together, where the output of one layer becomes the input of the next.\n",
    "\n",
    "            - **Convolutional Blocks:** The _features_ section consists of three blocks, each containing:\n",
    "                - **nn.Conv2d:** A 2D convolutional layer that extracts features from the input image. The parameters define input channels (3 for RGB images), output channels (64, 128, 256), kernel size (3x3), and padding (1 to maintain spatial dimensions).\n",
    "\n",
    "                - **nn.ReLU(inplace=True):** A _Rectified Linear Unit (ReLU)_ activation function, which introduces non-linearity. _inplace=True_ modifies the input directly, saving memory.\n",
    "\n",
    "                - **nn.MaxPool2d(kernel_size=2):** A 2D max pooling layer that downsamples the feature maps, reducing spatial dimensions and making the network more robust to small shifts in the input.\n",
    "\n",
    "        - **self.classifier = nn.Sequential(...):** This defines the \"classification\" part of the CNN.\n",
    "            - **nn.Flatten():** This layer flattens the multi-dimensional output from the features part into a 1D vector, preparing it for the fully connected layers. The input dimension 256 * 4 * 4 implies that after the features layers, the spatial dimensions of the feature maps are 4x4, and there are 256 channels.\n",
    "\n",
    "            - **nn.Linear(256 * 4 * 4, 1024):** A fully connected (linear) layer that takes the flattened features as input and outputs a 1024-dimensional vector.\n",
    "\n",
    "            - **nn.ReLU(inplace=True):** Another ReLU activation function.\n",
    "\n",
    "            - **nn.Linear(1024, 10):** The final fully connected layer that maps the 1024-dimensional vector to 10 output values, corresponding to the 10 classes in the CIFAR-10 dataset.\n",
    "\n",
    "* **def forward(self, x)::**\n",
    "\n",
    "This method defines the forward pass of the network, specifying how input data x flows through the layers.\n",
    "\n",
    "- **x = self.features(x):** The input x first passes through the feature extraction layers.\n",
    "- **x = self.classifier(x):** The output of the feature extraction is then passed through the classification layers.\n",
    "- **return x:** The final output of the network, representing the raw scores (logits) for each of the 10 classes, is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd448015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model for CIFAR10\n",
    "class CIFAR10CNN(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(CIFAR10CNN, self).__init__()\n",
    "       self.features = nn.Sequential(\n",
    "           nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "       )\n",
    "\n",
    "\n",
    "       self.classifier = nn.Sequential(\n",
    "           nn.Flatten(),\n",
    "           nn.Linear(256 * 4 * 4, 1024),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.Linear(1024, 10)\n",
    "       )\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.features(x)\n",
    "       x = self.classifier(x)\n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2648c",
   "metadata": {},
   "source": [
    "### Loading the CIFAR-10 Dataset\n",
    "\n",
    "The provided code defines a function load_data for loading and preprocessing the CIFAR-10 dataset in PyTorch, and then uses it to create data loaders for training and testing. It also creates a separate CPU-specific test data loader.\n",
    "\n",
    "### Understanding the Key Components of Loading the CIFAR-10 Dataset\n",
    "\n",
    "1. **load_data Function:**\n",
    "\n",
    "    - **Data Transformations:**\n",
    "\n",
    "        - **transform_train:** A sequence of transformations applied to the training data. This includes **RandomCrop** and **RandomHorizontalFlip** for data augmentation, **ToTensor** to convert images to PyTorch tensors, and **Normalize** to scale pixel values to a range of -1 to 1.\n",
    "\n",
    "        - **transform_test:** A sequence of transformations for the test data, applying **ToTensor** and **Normalize.**\n",
    "\n",
    "    - **Dataset Loading:**\n",
    "        - **train_dataset** and **test_dataset**: Loads the CIFAR-10 dataset, downloading it if not already present. The respective transformations (**transform_train** and **transform_test**) are applied.\n",
    "\n",
    "    - **Training Data Subset:**\n",
    "        - For faster demonstration, a subset of 10,000 samples is randomly selected from the **train_dataset** to create **train_subset**.\n",
    "\n",
    "    - **Data Loaders:**\n",
    "        - **train_loader:** Creates a **DataLoader** for the **train_subset** with a specified **batch_size**, shuffling the data, and using 2 worker processes for parallel data loading.\n",
    "\n",
    "        - **test_loader:** Creates a **DataLoader** for the **test_dataset** with the same **batch_size**, but without shuffling.\n",
    "\n",
    "    - **Return Values:**\n",
    "        - The function returns **train_loader** and **test_loader**.\n",
    "\n",
    "\n",
    "2. **Data Loading and CPU Dataloader:**\n",
    "    - **Loading Data:**\n",
    "        - The **load_data** function is called with **batch_size = 128** to obtain **train_loader** and **test_loader**.\n",
    "\n",
    "    - **CPU Dataloader for Quantization:**\n",
    "        - **test_loader_cpu:** A separate **DataLoader** is created specifically for the test set, configured to load data onto the CPU. This is explicitly mentioned as being for \"quantization operations,\" suggesting that subsequent parts of the code might involve quantizing a model, which often requires data to be on the CPU. The same **ToTensor** and **Normalize** transformations are applied as in **transform_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def load_data(batch_size=128):\n",
    "   transform_train = transforms.Compose([\n",
    "       transforms.RandomCrop(32, padding=4),\n",
    "       transforms.RandomHorizontalFlip(),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "   ])\n",
    "\n",
    "\n",
    "   transform_test = transforms.Compose([\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "   ])\n",
    "\n",
    "\n",
    "   train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "   test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "   # For faster demonstration, use a subset of training data\n",
    "   train_indices = list(range(len(train_dataset)))\n",
    "   random.shuffle(train_indices)\n",
    "   train_indices = train_indices[:10000]  # Use 10,000 samples for training\n",
    "\n",
    "\n",
    "   train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "\n",
    "   train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "   test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "   return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Load data\n",
    "batch_size = 128\n",
    "train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "\n",
    "# CPU dataloader for quantization operations\n",
    "test_loader_cpu = DataLoader(\n",
    "   datasets.CIFAR10('./data', train=False, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                    ])),\n",
    "   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539ff62",
   "metadata": {},
   "source": [
    "## Create Utilities for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_model_size(model):\n",
    "   \"\"\"Calculate the model size in MB\"\"\"\n",
    "   torch.save(model.state_dict(), \"temp.p\")\n",
    "   size = os.path.getsize(\"temp.p\")/1e6\n",
    "   os.remove('temp.p')\n",
    "   return size\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_batches=None):\n",
    "   \"\"\"Evaluate model accuracy\"\"\"\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = 0\n",
    "\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for i, (data, target) in enumerate(dataloader):\n",
    "           if num_batches is not None and i >= num_batches:\n",
    "               break\n",
    "\n",
    "\n",
    "           data, target = data.to(device), target.to(device)\n",
    "           outputs = model(data)\n",
    "           _, predicted = torch.max(outputs.data, 1)\n",
    "           total += target.size(0)\n",
    "           correct += (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "   return correct / total\n",
    "\n",
    "\n",
    "def measure_inference_time(model, dataloader, device, num_runs=100):\n",
    "   \"\"\"Measure inference time\"\"\"\n",
    "   model.eval()\n",
    "\n",
    "\n",
    "   # Get a batch for testing\n",
    "   data_iter = iter(dataloader)\n",
    "   batch, _ = next(data_iter)\n",
    "   batch = batch.to(device)\n",
    "\n",
    "\n",
    "   # Warmup\n",
    "   with torch.no_grad():\n",
    "       for _ in range(10):\n",
    "           _ = model(batch)\n",
    "\n",
    "\n",
    "   # Measure inference time\n",
    "   start_time = time.time()\n",
    "   with torch.no_grad():\n",
    "       for _ in range(num_runs):\n",
    "           _ = model(batch)\n",
    "   end_time = time.time()\n",
    "\n",
    "\n",
    "   avg_time = (end_time - start_time) / num_runs\n",
    "   return avg_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678e4cd",
   "metadata": {},
   "source": [
    "### Now Let's Train the Baseline Model\n",
    "\n",
    "\n",
    "The training uses `Cross-Entropy Loss` for classification determination\n",
    "\n",
    "Cross-entropy loss, also known as log loss, is a function used in machine learning to measure the difference between two probability distributions. In classification tasks, it quantifies how well a model's predicted probabilities align with the actual (true) labels. It's particularly useful when dealing with multiple classes, and it penalizes confident misclassifications more heavily than uncertain ones. \n",
    "\n",
    "* If a model predicts a high probability for the correct class, the loss is low. \n",
    "\n",
    "* If a model predicts a high probability for an incorrect class, the loss is high. \n",
    "\n",
    "* The logarithm in the formula means that the penalty for confident wrong predictions is much greater than for incorrect predictions with low confidence. \n",
    "\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Let's say you have a binary classification problem (e.g., cat vs. not cat). If the true label is \"cat\" (1), and the model predicts a probability of 0.9 for \"cat\", the cross-entropy loss will be relatively low. However, if the model predicts a probability of only 0.2 for \"cat\", the loss will be significantly higher, reflecting the model's poor confidence in the correct prediction. \n",
    "\n",
    "##### Why use it?\n",
    "Cross-entropy loss is a popular choice for classification problems because it:\n",
    "\n",
    "* **Encourages confident predictions:** It pushes the model to make strong predictions, reducing uncertainty. \n",
    "\n",
    "* **Penalizes misclassifications effectively:** It penalizes confident errors more severely than uncertain errors. \n",
    "\n",
    "* **Works well with _softmax activation_:** It's often used in conjunction with softmax in the output layer of neural networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, epochs=5):\n",
    "   \"\"\"Train the model\"\"\"\n",
    "   model.train()\n",
    "   training_losses = []\n",
    "\n",
    "\n",
    "   for epoch in range(epochs):\n",
    "       running_loss = 0.0\n",
    "       for i, (inputs, labels) in enumerate(train_loader):\n",
    "           inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "           # Zero the parameter gradients\n",
    "           optimizer.zero_grad()\n",
    "\n",
    "\n",
    "           # Forward + backward + optimize\n",
    "           outputs = model(inputs)\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "\n",
    "           # Print statistics\n",
    "           running_loss += loss.item()\n",
    "           if i % 100 == 99:  # Print every 100 mini-batches\n",
    "               print(f'Epoch {epoch+1}, Batch {i+1}: Loss = {running_loss/100:.4f}')\n",
    "               training_losses.append(running_loss/100)\n",
    "               running_loss = 0.0\n",
    "\n",
    "\n",
    "   return training_losses\n",
    "\n",
    "# Train and evaluate regular model (baseline)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL TRAINING AND EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "model_fp32 = CIFAR10CNN().to(train_device)\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "train_losses = train_model(\n",
    "   model_fp32, train_loader, optimizer, criterion, train_device, epochs=3\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on the same device used for training\n",
    "print(\"Evaluating baseline model...\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, train_device)\n",
    "fp32_inference_time = measure_inference_time(model_fp32, test_loader, train_device)\n",
    "fp32_size = get_model_size(model_fp32)\n",
    "print(f\"FP32 Model Accuracy: {fp32_accuracy:.4f}\")\n",
    "print(f\"FP32 Inference Time per batch: {fp32_inference_time*1000:.2f} ms\")\n",
    "print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "\n",
    "\n",
    "# Save the model state dict for later use\n",
    "torch.save(model_fp32.state_dict(), \"fp32_model.pth\")\n",
    "\n",
    "\n",
    "# Move model to CPU for quantization operations\n",
    "model_fp32_cpu = CIFAR10CNN()\n",
    "model_fp32_cpu.load_state_dict(model_fp32.state_dict())\n",
    "model_fp32_cpu.eval()\n",
    "\n",
    "\n",
    "# For fair comparison, also measure FP32 performance on CPU\n",
    "fp32_cpu_inference_time = measure_inference_time(model_fp32_cpu, test_loader_cpu, \"cpu\")\n",
    "print(f\"FP32 Inference Time on CPU: {fp32_cpu_inference_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9b048",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Dynamic Quantization\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "This notebook assumes you've trained the baseline model and have the following variables available:\n",
    "\n",
    "- model_fp32_cpu: The baseline CPU model\n",
    "- test_loader_cpu: DataLoader for CPU testing\n",
    "- fp32_accuracy: Baseline accuracy\n",
    "- fp32_size: Baseline model size\n",
    "- fp32_cpu_inference_time: Baseline CPU inference time\n",
    "\n",
    "\n",
    "#### Special note for macOS users\n",
    "Ensure PyTorch is Built with Quantization Backends: Verify that your PyTorch installation includes the necessary quantization backends. For CPU quantization, FBGEMM (for x86) and QNNPACK (for ARM) are common. If PyTorch was installed from source, ensure these backends were enabled during compilation. If installed via pip or conda, confirm you are using a build that includes them.\n",
    "\n",
    "Use \n",
    "```\n",
    "# Example for checking FBGEMM support (if applicable)\n",
    "    import torch\n",
    "    print(torch.backends.quantized.engine_list())\n",
    "\n",
    "```\n",
    "\n",
    "Explicitly Set the Quantization Backend: Although dynamic quantization often automatically selects a backend, explicitly setting it can sometimes help fix this error exception\n",
    "\n",
    "`RuntimeError: Didn't find engine for operation quantized::linear_prepack NoQEngine`\n",
    "\n",
    "\n",
    "#### Example: Set qnnpack as the default engine\n",
    "\n",
    "```bash\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Training Dynamic Quantization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POST-TRAINING DYNAMIC QUANTIZATION (CPU)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Apply dynamic quantization on CPU model\n",
    "print(\"Applying dynamic quantization...\")\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "model_dynamic = torch.quantization.quantize_dynamic(\n",
    "   model_fp32_cpu,  # Use CPU model\n",
    "   {nn.Linear, nn.Conv2d},  # Quantize both linear and conv layers\n",
    "   dtype=torch.qint8  # Use 8-bit integers\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate dynamically quantized model (always on CPU)\n",
    "print(\"Evaluating dynamically quantized model on CPU...\")\n",
    "dynamic_accuracy = evaluate_model(model_dynamic, test_loader_cpu, \"cpu\")\n",
    "dynamic_inference_time = measure_inference_time(model_dynamic, test_loader_cpu, \"cpu\")\n",
    "dynamic_size = get_model_size(model_dynamic)\n",
    "\n",
    "\n",
    "print(f\"Dynamic Quantized Model Accuracy: {dynamic_accuracy:.4f}\")\n",
    "print(f\"Accuracy Change: {(dynamic_accuracy - fp32_accuracy)*100:.2f}%\")\n",
    "print(f\"Dynamic Quantized Inference Time (CPU): {dynamic_inference_time*1000:.2f} ms\")\n",
    "print(f\"FP32 Inference Time (CPU): {fp32_cpu_inference_time*1000:.2f} ms\")\n",
    "print(f\"Speedup vs FP32 on CPU: {fp32_cpu_inference_time/dynamic_inference_time:.2f}x\")\n",
    "print(f\"Dynamic Quantized Model Size: {dynamic_size:.2f} MB\")\n",
    "print(f\"Size Reduction: {(1 - dynamic_size/fp32_size)*100:.2f}%\")\n",
    "\n",
    "\n",
    "# Visualization for Dynamic Quantization vs FP32\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "models = ['FP32', 'Dynamic Quantization']\n",
    "accuracies = [fp32_accuracy, dynamic_accuracy]\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(min(accuracies) - 0.05, 1.0)\n",
    "\n",
    "\n",
    "# Size comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "sizes = [fp32_size, dynamic_size]\n",
    "bars = plt.bar(models, sizes, color=['blue', 'green'])\n",
    "plt.title('Model Size (MB)')\n",
    "plt.ylabel('Size (MB)')\n",
    "# Add percentage reduction label\n",
    "reduction = (1 - dynamic_size/fp32_size) * 100\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, dynamic_size + 0.5,\n",
    "        f\"{reduction:.1f}% reduction\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Inference time comparison (all on CPU for fair comparison)\n",
    "plt.subplot(1, 3, 3)\n",
    "times = [fp32_cpu_inference_time*1000, dynamic_inference_time*1000]\n",
    "bars = plt.bar(models, times, color=['blue', 'green'])\n",
    "plt.title('Inference Time on CPU (ms)')\n",
    "plt.ylabel('Time (ms)')\n",
    "# Add speedup label\n",
    "speedup = fp32_cpu_inference_time/dynamic_inference_time\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, dynamic_inference_time*1000 + 1,\n",
    "        f\"{speedup:.1f}x faster\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dynamic_quantization_comparison.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482e8fe",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Static Quantization\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "This notebook assumes you've trained the baseline model and have the following variables available:\n",
    "\n",
    "- model_fp32_cpu: The baseline CPU model\n",
    "- test_loader_cpu: DataLoader for CPU testing\n",
    "- fp32_accuracy: Baseline accuracy\n",
    "- fp32_size: Baseline model size\n",
    "- fp32_cpu_inference_time: Baseline CPU inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Quantization\n",
    "\n",
    "# Define a PTQ-ready model with proper quantization support\n",
    "class QuantizablePTQCIFAR10CNN(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(QuantizablePTQCIFAR10CNN, self).__init__()\n",
    "       # Quantization stubs\n",
    "       self.quant = torch.quantization.QuantStub()\n",
    "       self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "\n",
    "       # Feature extraction layers\n",
    "       self.features = nn.Sequential(\n",
    "           nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "       )\n",
    "\n",
    "\n",
    "       # Classification layers\n",
    "       self.classifier = nn.Sequential(\n",
    "           nn.Flatten(),\n",
    "           nn.Linear(256 * 4 * 4, 1024),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.Linear(1024, 10)\n",
    "       )\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.quant(x)\n",
    "       x = self.features(x)\n",
    "       x = self.classifier(x)\n",
    "       x = self.dequant(x)\n",
    "       return x\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POST-TRAINING STATIC QUANTIZATION (CPU)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Create quantizable model on CPU\n",
    "model_static = QuantizablePTQCIFAR10CNN()\n",
    "# Load state dict from the trained model\n",
    "model_static.load_state_dict(model_fp32.state_dict())\n",
    "model_static.eval()\n",
    "\n",
    "\n",
    "# Configure static quantization\n",
    "model_static.qconfig = torch.quantization.get_default_qconfig(\"qnnpack\") # for x86 use \"fbgemm\"\n",
    "\n",
    "\n",
    "# Prepare for static quantization\n",
    "print(\"Preparing model for static quantization...\")\n",
    "model_static_prepared = torch.quantization.prepare(model_static)\n",
    "\n",
    "\n",
    "# Calibration function to determine optimal quantization parameters\n",
    "def calibrate(model, data_loader, num_batches=10):\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "       for i, (data, _) in enumerate(data_loader):\n",
    "           if i >= num_batches:\n",
    "               break\n",
    "           # No need to explicitly move to CPU since model is on CPU\n",
    "           _ = model(data)\n",
    "\n",
    "\n",
    "# Calibrate with sample data\n",
    "print(\"Calibrating with test data...\")\n",
    "calibrate(model_static_prepared, test_loader_cpu)\n",
    "\n",
    "\n",
    "# Convert to fully quantized model\n",
    "print(\"Converting to quantized model...\")\n",
    "model_static_quantized = torch.quantization.convert(model_static_prepared)\n",
    "\n",
    "# Evaluate statically quantized model on CPU\n",
    "print(\"Evaluating statically quantized model on CPU...\")\n",
    "static_accuracy = evaluate_model(model_static_quantized, test_loader_cpu, \"cpu\")\n",
    "static_inference_time = measure_inference_time(model_static_quantized, test_loader_cpu, \"cpu\")\n",
    "static_size = get_model_size(model_static_quantized)\n",
    "\n",
    "\n",
    "print(f\"Static Quantized Model Accuracy: {static_accuracy:.4f}\")\n",
    "print(f\"Accuracy Change: {(static_accuracy - fp32_accuracy)*100:.2f}%\")\n",
    "print(f\"Static Quantized Inference Time (CPU): {static_inference_time*1000:.2f} ms\")\n",
    "print(f\"Speedup vs FP32 on CPU: {fp32_cpu_inference_time/static_inference_time:.2f}x\")\n",
    "print(f\"Static Quantized Model Size: {static_size:.2f} MB\")\n",
    "print(f\"Size Reduction: {(1 - static_size/fp32_size)*100:.2f}%\")\n",
    "\n",
    "\n",
    "# Visualization for Static Quantization vs FP32\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "models = ['FP32', 'Static Quantization']\n",
    "accuracies = [fp32_accuracy, static_accuracy]\n",
    "plt.bar(models, accuracies, color=['blue', 'orange'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(min(accuracies) - 0.05, 1.0)\n",
    "\n",
    "\n",
    "# Size comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "sizes = [fp32_size, static_size]\n",
    "bars = plt.bar(models, sizes, color=['blue', 'orange'])\n",
    "plt.title('Model Size (MB)')\n",
    "plt.ylabel('Size (MB)')\n",
    "# Add percentage reduction label\n",
    "reduction = (1 - static_size/fp32_size) * 100\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, static_size + 0.5,\n",
    "        f\"{reduction:.1f}% reduction\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Inference time comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "times = [fp32_cpu_inference_time*1000, static_inference_time*1000]\n",
    "bars = plt.bar(models, times, color=['blue', 'orange'])\n",
    "plt.title('Inference Time on CPU (ms)')\n",
    "plt.ylabel('Time (ms)')\n",
    "# Add speedup label\n",
    "speedup = fp32_cpu_inference_time/static_inference_time\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, static_inference_time*1000 + 1,\n",
    "        f\"{speedup:.1f}x faster\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('static_quantization_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda88614",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Quantization Aware Training\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "This notebook assumes you've trained the baseline model and have the following variables available:\n",
    "\n",
    "- model_fp32_cpu: The baseline CPU model\n",
    "- test_loader_cpu: DataLoader for CPU testing\n",
    "- fp32_accuracy: Baseline accuracy\n",
    "- fp32_size: Baseline model size\n",
    "- fp32_cpu_inference_time: Baseline CPU inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a QAT-ready model with proper quantization support\n",
    "class QuantizableCIFAR10CNN(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(QuantizableCIFAR10CNN, self).__init__()\n",
    "       # Quantization stubs\n",
    "       self.quant = torch.quantization.QuantStub()\n",
    "       self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "\n",
    "       # Feature extraction layers\n",
    "       self.features = nn.Sequential(\n",
    "           nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "           nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.MaxPool2d(kernel_size=2),\n",
    "       )\n",
    "\n",
    "\n",
    "       # Classification layers\n",
    "       self.classifier = nn.Sequential(\n",
    "           nn.Flatten(),\n",
    "           nn.Linear(256 * 4 * 4, 1024),\n",
    "           nn.ReLU(inplace=True),\n",
    "           nn.Linear(1024, 10)\n",
    "       )\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.quant(x)\n",
    "       x = self.features(x)\n",
    "       x = self.classifier(x)\n",
    "       x = self.dequant(x)\n",
    "       return x\n",
    "\n",
    "\n",
    "\n",
    "# Quantization-Aware Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUANTIZATION-AWARE TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Create new model for QAT\n",
    "model_qat = QuantizableCIFAR10CNN().to(train_device)\n",
    "model_qat.load_state_dict(model_fp32.state_dict())\n",
    "\n",
    "\n",
    "# Configure optimizer with lower learning rate for fine-tuning\n",
    "optimizer_qat = optim.Adam(model_qat.parameters(), lr=0.0001)\n",
    "criterion_qat = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Initial training in FP32\n",
    "print(\"Initial training phase (FP32)...\")\n",
    "model_qat.train()\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "   if i >= 200:  # Just a few batches for initial FP32 training\n",
    "       break\n",
    "\n",
    "\n",
    "   inputs, labels = inputs.to(train_device), labels.to(train_device)\n",
    "   optimizer_qat.zero_grad()\n",
    "   outputs = model_qat(inputs)\n",
    "   loss = criterion_qat(outputs, labels)\n",
    "   loss.backward()\n",
    "   optimizer_qat.step()\n",
    "\n",
    "\n",
    "# Move to CPU for QAT preparation\n",
    "model_qat = model_qat.cpu()\n",
    "\n",
    "\n",
    "# Prepare for QAT\n",
    "print(\"Preparing for QAT...\")\n",
    "model_qat.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(model_qat, inplace=True)\n",
    "\n",
    "\n",
    "# Move back to training device\n",
    "model_qat = model_qat.to(train_device)\n",
    "\n",
    "\n",
    "# QAT training\n",
    "print(\"QAT training phase...\")\n",
    "model_qat.train()\n",
    "qat_losses = []\n",
    "for epoch in range(2):  # 2 epochs of QAT\n",
    "   running_loss = 0.0\n",
    "   for i, (inputs, labels) in enumerate(train_loader):\n",
    "       inputs, labels = inputs.to(train_device), labels.to(train_device)\n",
    "\n",
    "\n",
    "       optimizer_qat.zero_grad()\n",
    "       outputs = model_qat(inputs)\n",
    "       loss = criterion_qat(outputs, labels)\n",
    "       loss.backward()\n",
    "       optimizer_qat.step()\n",
    "\n",
    "\n",
    "       running_loss += loss.item()\n",
    "       if i % 100 == 99:\n",
    "           print(f'QAT Epoch {epoch+1}, Batch {i+1}: Loss = {running_loss/100:.4f}')\n",
    "           qat_losses.append(running_loss/100)\n",
    "           running_loss = 0.0\n",
    "\n",
    "\n",
    "       if i >= 300:  # Limit batches for demonstration\n",
    "           break\n",
    "\n",
    "\n",
    "# Evaluate QAT model before final conversion\n",
    "print(\"Evaluating QAT model before conversion...\")\n",
    "qat_fp32_accuracy = evaluate_model(model_qat, test_loader, train_device)\n",
    "print(f\"QAT Model Accuracy (before conversion): {qat_fp32_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Convert QAT model to fully quantized model (on CPU)\n",
    "print(\"Converting QAT model to deployable quantized model...\")\n",
    "model_qat = model_qat.cpu()\n",
    "model_qat_quantized = torch.quantization.convert(model_qat)\n",
    "\n",
    "\n",
    "# Evaluate final QAT model\n",
    "print(\"Evaluating final QAT model on CPU...\")\n",
    "qat_accuracy = evaluate_model(model_qat_quantized, test_loader_cpu, \"cpu\")\n",
    "qat_inference_time = measure_inference_time(model_qat_quantized, test_loader_cpu, \"cpu\")\n",
    "qat_size = get_model_size(model_qat_quantized)\n",
    "\n",
    "\n",
    "print(f\"QAT Model Accuracy: {qat_accuracy:.4f}\")\n",
    "print(f\"Accuracy Change vs FP32: {(qat_accuracy - fp32_accuracy)*100:.2f}%\")\n",
    "print(f\"QAT Model Inference Time (CPU): {qat_inference_time*1000:.2f} ms\")\n",
    "print(f\"Speedup vs FP32 on CPU: {fp32_cpu_inference_time/qat_inference_time:.2f}x\")\n",
    "print(f\"QAT Model Size: {qat_size:.2f} MB\")\n",
    "print(f\"Size Reduction vs FP32: {(1 - qat_size/fp32_size)*100:.2f}%\")\n",
    "\n",
    "\n",
    "# Visualization for QAT vs FP32\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "models = ['FP32', 'QAT']\n",
    "accuracies = [fp32_accuracy, qat_accuracy]\n",
    "plt.bar(models, accuracies, color=['blue', 'red'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(min(accuracies) - 0.05, 1.0)\n",
    "\n",
    "\n",
    "# Size comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "sizes = [fp32_size, qat_size]\n",
    "bars = plt.bar(models, sizes, color=['blue', 'red'])\n",
    "plt.title('Model Size (MB)')\n",
    "plt.ylabel('Size (MB)')\n",
    "# Add percentage reduction label\n",
    "reduction = (1 - qat_size/fp32_size) * 100\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, qat_size + 0.5,\n",
    "        f\"{reduction:.1f}% reduction\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Inference time comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "times = [fp32_cpu_inference_time*1000, qat_inference_time*1000]\n",
    "bars = plt.bar(models, times, color=['blue', 'red'])\n",
    "plt.title('Inference Time on CPU (ms)')\n",
    "plt.ylabel('Time (ms)')\n",
    "# Add speedup label\n",
    "speedup = fp32_cpu_inference_time/qat_inference_time\n",
    "plt.text(bars[1].get_x() + bars[1].get_width()/2, qat_inference_time*1000 + 1,\n",
    "        f\"{speedup:.1f}x faster\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('qat_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed69d44",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Comparing Quantization Techniques\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "This notebook assumes you've run all previous parts and have these variables:\n",
    "\n",
    "- fp32_accuracy, fp32_size, fp32_cpu_inference_time (baseline)\n",
    "- dynamic_accuracy, dynamic_size, dynamic_inference_time (dynamic quantization)\n",
    "- static_accuracy, static_size, static_inference_time (static quantization)\n",
    "- qat_accuracy, qat_size, qat_inference_time (QAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65118108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUANTIZATION COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model Type':<25} {'Size (MB)':<15} {'Accuracy':<15} {'Inference (ms)':<15} {'Speedup':<10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "# For fair comparison, use CPU times for all models\n",
    "print(f\"{'Original FP32 (CPU)':<25} {fp32_size:<15.2f} {fp32_accuracy:<15.4f} {fp32_cpu_inference_time*1000:<15.2f} {1.0:<10.2f}x\")\n",
    "print(f\"{'Dynamic Quantization':<25} {dynamic_size:<15.2f} {dynamic_accuracy:<15.4f} {dynamic_inference_time*1000:<15.2f} {fp32_cpu_inference_time/dynamic_inference_time:<10.2f}x\")\n",
    "print(f\"{'Static Quantization':<25} {static_size:<15.2f} {static_accuracy:<15.4f} {static_inference_time*1000:<15.2f} {fp32_cpu_inference_time/static_inference_time:<10.2f}x\")\n",
    "print(f\"{'QAT':<25} {qat_size:<15.2f} {qat_accuracy:<15.4f} {qat_inference_time*1000:<15.2f} {fp32_cpu_inference_time/qat_inference_time:<10.2f}x\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Create a comprehensive visualization\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    "# Model Size Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = ['FP32', 'Dynamic', 'Static', 'QAT']\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "sizes = [fp32_size, dynamic_size, static_size, qat_size]\n",
    "bars = plt.bar(models, sizes, color=colors)\n",
    "plt.title('Model Size (MB)', fontsize=14)\n",
    "plt.ylabel('Size (MB)', fontsize=12)\n",
    "\n",
    "\n",
    "# Add reduction percentages\n",
    "for i, bar in enumerate(bars[1:], 1):\n",
    "   reduction = (1 - sizes[i]/sizes[0]) * 100\n",
    "   plt.text(bar.get_x() + bar.get_width()/2, sizes[i] + 0.5,\n",
    "            f\"{reduction:.1f}%\\nreduction\",\n",
    "            ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "\n",
    "# Accuracy Comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "accuracies = [fp32_accuracy, dynamic_accuracy, static_accuracy, qat_accuracy]\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "plt.title('Model Accuracy', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(min(accuracies) - 0.05, 1.0)\n",
    "\n",
    "\n",
    "# Add accuracy change percentages\n",
    "for i, bar in enumerate(bars[1:], 1):\n",
    "   change = (accuracies[i] - accuracies[0]) * 100\n",
    "   color = 'green' if change >= 0 else 'red'\n",
    "   prefix = '+' if change >= 0 else ''\n",
    "   plt.text(bar.get_x() + bar.get_width()/2, accuracies[i] + 0.01,\n",
    "            f\"{prefix}{change:.2f}%\",\n",
    "            ha='center', va='bottom', fontsize=11, color=color)\n",
    "\n",
    "\n",
    "# Inference Time Comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "times = [fp32_cpu_inference_time*1000, dynamic_inference_time*1000,\n",
    "        static_inference_time*1000, qat_inference_time*1000]\n",
    "bars = plt.bar(models, times, color=colors)\n",
    "plt.title('Inference Time on CPU (ms)', fontsize=14)\n",
    "plt.ylabel('Time (ms)', fontsize=12)\n",
    "\n",
    "\n",
    "# Add speedup labels\n",
    "for i, bar in enumerate(bars[1:], 1):\n",
    "   speedup = fp32_cpu_inference_time/[dynamic_inference_time, static_inference_time, qat_inference_time][i-1]\n",
    "   plt.text(bar.get_x() + bar.get_width()/2, times[i]/2,\n",
    "            f\"{speedup:.2f}x\\nfaster\",\n",
    "            ha='center', va='center', fontsize=11, color='white', weight='bold')\n",
    "\n",
    "\n",
    "# Size-Speed-Accuracy Trade-off\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(sizes, times, s=np.array(accuracies)*500, c=colors, alpha=0.7)\n",
    "\n",
    "\n",
    "# Add labels for each point\n",
    "for i, model in enumerate(models):\n",
    "   plt.annotate(model,\n",
    "              (sizes[i], times[i]),\n",
    "              textcoords=\"offset points\",\n",
    "              xytext=(0,10),\n",
    "              ha='center', fontsize=12)\n",
    "\n",
    "\n",
    "plt.title('Size vs. Speed vs. Accuracy Trade-off', fontsize=14)\n",
    "plt.xlabel('Model Size (MB)', fontsize=12)\n",
    "plt.ylabel('Inference Time (ms)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Add a legend for bubble size\n",
    "plt.figtext(0.85, 0.25, \"Bubble size represents accuracy\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quantization_methods_comparison.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
