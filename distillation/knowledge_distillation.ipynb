{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca88ec64",
   "metadata": {},
   "source": [
    "# Knowledge Distillation Approach Using PyTorch\n",
    "\n",
    "The general work flow:\n",
    "<p>\n",
    "  <img alt=Knowledge Distillation Workflow\" src=\"distillation_workflow.png\" width=\"450\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "[img source: knowledge distillation section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c85698",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b009f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6c96b",
   "metadata": {},
   "source": [
    "### Set Random Seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03808546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca7599",
   "metadata": {},
   "source": [
    "### Do an initial check if Compute Unified Device Architecture (CUDA) is available\n",
    "\n",
    "Checking if CUDA is available is a crucial step in applications, particularly in deep learning and high-performance computing, for several reasons: \n",
    "\n",
    "* Enabling GPU Acceleration:\n",
    "\n",
    "    CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and API that allows software to leverage the power of NVIDIA GPUs for general-purpose computing. Checking for its availability determines whether your program can offload computationally intensive tasks to the GPU, leading to significant speedups compared to CPU-only execution.\n",
    "\n",
    "* Conditional Code Execution:\n",
    "\n",
    "    By checking for CUDA availability, you can write code that dynamically adapts to the hardware environment. If a CUDA-enabled GPU is present, your program can utilize GPU-specific operations and data structures. If not, it can gracefully fall back to CPU implementations or inform the user about the lack of GPU support. This prevents errors and ensures your application can run on various systems.\n",
    "\n",
    "* Resource Management:\n",
    "\n",
    "    Knowing if CUDA is available allows you to manage resources effectively. If a GPU is present, you can allocate memory on the device and perform computations there. If not, you avoid attempting to access non-existent GPU resources, which would lead to errors.\n",
    "\n",
    "* Error Prevention and Debugging:\n",
    "\n",
    "    Explicitly checking for CUDA availability helps in identifying and preventing issues related to missing or improperly configured CUDA installations or incompatible GPU drivers. If the check fails, it provides an immediate indication that GPU acceleration is not possible, guiding troubleshooting efforts.\n",
    "\n",
    "* Optimized Performance:\n",
    "\n",
    "    Many deep learning frameworks and libraries are designed to leverage CUDA for optimal performance. Verifying CUDA availability ensures that these frameworks can utilize the intended hardware acceleration, leading to faster training times and inference speeds for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f939869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for training: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "print(f\"Using device for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bb5e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Building the Teacher and Student Models: Model Definition\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "  <img alt=\"The Teacher - Student Architecture\" src=\"teacher_student_architecture.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: knowledge distillation section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb197c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define teacher and student models\n",
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"A larger model to act as the teacher\"\"\"\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "\n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"Get intermediate features for additional distillation\"\"\"\n",
    "        features = []\n",
    "\n",
    "        # Extract features from each layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"A smaller model to be trained via knowledge distillation\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        # Convolutional layers (fewer filters)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers (smaller)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "\n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"Get intermediate features for additional distillation\"\"\"\n",
    "        features = []\n",
    "\n",
    "        # Extract features from each layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        features.append(x)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d2b18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Knowledge Distillation Loss\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "  <img alt=\"Feature Distillation\" src=\"feature_distillation.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[img source: knowledge distillation section](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge distillation loss\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=2.0):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for distillation loss vs standard loss\n",
    "        self.temperature = temperature  # Temperature for softening probability distributions\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Standard cross-entropy loss\n",
    "        hard_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "        # Distillation loss: KL-divergence between soft targets from teacher and student\n",
    "        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)\n",
    "\n",
    "        # Combine the two losses\n",
    "        loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Feature distillation loss - optional enhancement\n",
    "class FeatureDistillationLoss(nn.Module):\n",
    "    def __init__(self, beta=0.1):\n",
    "        super(FeatureDistillationLoss, self).__init__()\n",
    "        self.beta = beta  # Weight for feature distillation\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # We'll implement a simple L2 distance for feature matching\n",
    "        # For simplicity, we only use the last feature map from each\n",
    "        loss = 0\n",
    "\n",
    "        # Adapt student feature dimensions to match teacher's\n",
    "        student_last_feature = student_features[-1]\n",
    "        teacher_last_feature = teacher_features[-1]\n",
    "\n",
    "        # Compute the mean squared error loss\n",
    "        feat_loss = F.mse_loss(student_last_feature, teacher_last_feature)\n",
    "\n",
    "        return self.beta * feat_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc2ed0",
   "metadata": {},
   "source": [
    "### Loading the Modified National Institute of Standards and Technology (MNIST) Dataset\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a widely used dataset in the field of machine learning, particularly for image recognition and classification tasks.\n",
    "\n",
    "#### Key characteristics of the MNIST dataset:\n",
    "\n",
    "* **Handwritten Digits:**\n",
    "    - It consists of a large collection of grayscale images of handwritten digits (0-9).\n",
    "\n",
    "* **Image Dimensions:**\n",
    "    - Each image is a 28x28 pixel grayscale image.\n",
    "\n",
    "* **Dataset Size:**\n",
    "    - It comprises a training set of 60,000 examples and a test set of 10,000 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a10f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e79f30",
   "metadata": {},
   "source": [
    "# Train Teacher and Student Models (standard training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f01fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train teacher model (standard training)\n",
    "def train_teacher(model, trainloader, epochs=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Training teacher model...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished training teacher model')\n",
    "    return model\n",
    "\n",
    "# Train student model with knowledge distillation\n",
    "def train_student_with_distillation(student_model, teacher_model, trainloader,\n",
    "                                   epochs=3, alpha=0.5, temperature=2.0, beta=0.0):\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()  # Teacher model is fixed\n",
    "\n",
    "    distill_criterion = DistillationLoss(alpha=alpha, temperature=temperature)\n",
    "    feature_criterion = FeatureDistillationLoss(beta=beta) if beta > 0 else None\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Training student model with distillation...\")\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get outputs from both models\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "                if beta > 0:\n",
    "                    teacher_features = teacher_model.get_features(inputs)\n",
    "\n",
    "            student_outputs = student_model(inputs)\n",
    "            if beta > 0:\n",
    "                student_features = student_model.get_features(inputs)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = distill_criterion(student_outputs, teacher_outputs, labels)\n",
    "\n",
    "            # Add feature matching loss if requested\n",
    "            if beta > 0:\n",
    "                feature_loss = feature_criterion(student_features, teacher_features)\n",
    "                loss += feature_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished training student model')\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f9773",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine-Tuning Student Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1a844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune student model on task-specific data\n",
    "def fine_tune_student(model, trainloader, epochs=2):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Lower learning rate for fine-tuning\n",
    "\n",
    "    print(\"Fine-tuning student model...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Fine-tuning Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished fine-tuning student model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd3cc2",
   "metadata": {},
   "source": [
    "## Utils for Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ca90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy\n",
    "def evaluate_model(model, testloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Measure inference time\n",
    "def measure_inference_time(model, testloader, num_batches=10):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Warm-up\n",
    "    for i, (images, _) in enumerate(testloader):\n",
    "        if i > 5:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(images)\n",
    "\n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(testloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "            batch_count += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / batch_count\n",
    "\n",
    "    return avg_time\n",
    "\n",
    "# Get model size\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp_model.pt\")\n",
    "    size_mb = os.path.getsize(\"temp_model.pt\") / (1024 * 1024)\n",
    "    os.remove(\"temp_model.pt\")\n",
    "    return size_mb\n",
    "\n",
    "# Save model predictions for further analysis\n",
    "def save_predictions(model, testloader, filename):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    np.savez(filename, predictions=np.array(all_preds), labels=np.array(all_labels))\n",
    "\n",
    "# Visualize predictions\n",
    "def plot_confusion_matrix(model_name, predictions_file):\n",
    "    data = np.load(predictions_file)\n",
    "    preds = data['predictions']\n",
    "    labels = data['labels']\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{model_name}_confusion_matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7996b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Main Function \n",
    "\n",
    "---\n",
    "\n",
    "Run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d70aea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    trainloader, testloader = load_data()\n",
    "\n",
    "    # Create and train teacher model\n",
    "    teacher_model = TeacherModel()\n",
    "    teacher_params = count_parameters(teacher_model)\n",
    "    print(f\"Teacher model has {teacher_params:,} parameters\")\n",
    "\n",
    "    # Check if a pretrained model exists to save time\n",
    "    if os.path.exists('teacher_model.pt'):\n",
    "        print(\"Loading pre-trained teacher model...\")\n",
    "        teacher_model.load_state_dict(torch.load('teacher_model.pt'))\n",
    "    else:\n",
    "        teacher_model = train_teacher(teacher_model, trainloader, epochs=3)\n",
    "        torch.save(teacher_model.state_dict(), 'teacher_model.pt')\n",
    "\n",
    "    # Evaluate teacher model\n",
    "    teacher_accuracy = evaluate_model(teacher_model, testloader)\n",
    "    teacher_inference_time = measure_inference_time(teacher_model, testloader)\n",
    "    teacher_size = get_model_size(teacher_model)\n",
    "\n",
    "    print(\"\\n--- Teacher Model Metrics ---\")\n",
    "    print(f\"Accuracy: {teacher_accuracy:.2f}%\")\n",
    "    print(f\"Parameters: {teacher_params:,}\")\n",
    "    print(f\"Inference Time: {teacher_inference_time*1000:.2f} ms per batch\")\n",
    "    print(f\"Model Size: {teacher_size:.2f} MB\")\n",
    "\n",
    "    # Create student model\n",
    "    student_model = StudentModel()\n",
    "    student_params = count_parameters(student_model)\n",
    "    print(f\"\\nStudent model has {student_params:,} parameters\")\n",
    "    print(f\"Parameter reduction: {(1 - student_params/teacher_params)*100:.1f}%\")\n",
    "\n",
    "    # Train student model without distillation (for comparison)\n",
    "    standard_student = copy.deepcopy(student_model)\n",
    "    if os.path.exists('standard_student.pt'):\n",
    "        print(\"Loading pre-trained standard student model...\")\n",
    "        standard_student.load_state_dict(torch.load('standard_student.pt'))\n",
    "    else:\n",
    "        standard_student = train_teacher(standard_student, trainloader, epochs=3)\n",
    "        torch.save(standard_student.state_dict(), 'standard_student.pt')\n",
    "\n",
    "    # Evaluate standard student\n",
    "    standard_student_accuracy = evaluate_model(standard_student, testloader)\n",
    "    standard_student_time = measure_inference_time(standard_student, testloader)\n",
    "    standard_student_size = get_model_size(standard_student)\n",
    "\n",
    "    print(\"\\n--- Standard Student Model Metrics ---\")\n",
    "    print(f\"Accuracy: {standard_student_accuracy:.2f}%\")\n",
    "    print(f\"Parameters: {student_params:,}\")\n",
    "    print(f\"Inference Time: {standard_student_time*1000:.2f} ms per batch\")\n",
    "    print(f\"Model Size: {standard_student_size:.2f} MB\")\n",
    "\n",
    "    # Train student with knowledge distillation\n",
    "    distilled_student = copy.deepcopy(student_model)\n",
    "\n",
    "    if os.path.exists('distilled_student.pt'):\n",
    "        print(\"Loading pre-trained distilled student model...\")\n",
    "        distilled_student.load_state_dict(torch.load('distilled_student.pt'))\n",
    "    else:\n",
    "        distilled_student = train_student_with_distillation(\n",
    "            distilled_student, teacher_model, trainloader,\n",
    "            epochs=3, alpha=0.5, temperature=4.0)\n",
    "        torch.save(distilled_student.state_dict(), 'distilled_student.pt')\n",
    "\n",
    "    # Evaluate distilled student\n",
    "    distilled_accuracy = evaluate_model(distilled_student, testloader)\n",
    "    distilled_time = measure_inference_time(distilled_student, testloader)\n",
    "    distilled_size = get_model_size(distilled_student)\n",
    "\n",
    "    print(\"\\n--- Distilled Student Model Metrics ---\")\n",
    "    print(f\"Accuracy: {distilled_accuracy:.2f}%\")\n",
    "    print(f\"Parameters: {student_params:,}\")\n",
    "    print(f\"Inference Time: {distilled_time*1000:.2f} ms per batch\")\n",
    "    print(f\"Model Size: {distilled_size:.2f} MB\")\n",
    "\n",
    "    # Fine-tune the distilled student\n",
    "    fine_tuned_student = copy.deepcopy(distilled_student)\n",
    "\n",
    "    if os.path.exists('fine_tuned_student.pt'):\n",
    "        print(\"Loading pre-trained fine-tuned student model...\")\n",
    "        fine_tuned_student.load_state_dict(torch.load('fine_tuned_student.pt'))\n",
    "    else:\n",
    "        fine_tuned_student = fine_tune_student(fine_tuned_student, trainloader, epochs=2)\n",
    "        torch.save(fine_tuned_student.state_dict(), 'fine_tuned_student.pt')\n",
    "\n",
    "    # Evaluate fine-tuned student\n",
    "    fine_tuned_accuracy = evaluate_model(fine_tuned_student, testloader)\n",
    "    fine_tuned_time = measure_inference_time(fine_tuned_student, testloader)\n",
    "\n",
    "    print(\"\\n--- Fine-tuned Student Model Metrics ---\")\n",
    "    print(f\"Accuracy: {fine_tuned_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Improvement from Distillation: {distilled_accuracy - standard_student_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Improvement from Fine-tuning: {fine_tuned_accuracy - distilled_accuracy:.2f}%\")\n",
    "    print(f\"Inference Time: {fine_tuned_time*1000:.2f} ms per batch\")\n",
    "\n",
    "    # Save predictions for analysis\n",
    "    save_predictions(teacher_model, testloader, 'teacher_preds.npz')\n",
    "    save_predictions(standard_student, testloader, 'standard_student_preds.npz')\n",
    "    save_predictions(distilled_student, testloader, 'distilled_student_preds.npz')\n",
    "    save_predictions(fine_tuned_student, testloader, 'fine_tuned_student_preds.npz')\n",
    "\n",
    "    # Comparison summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"KNOWLEDGE DISTILLATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Model':<25} {'Accuracy':<10} {'Size (MB)':<12} {'Inference (ms)':<15} {'Parameters':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Teacher':<25} {teacher_accuracy:<10.2f} {teacher_size:<12.2f} {teacher_inference_time*1000:<15.2f} {teacher_params:,}\")\n",
    "    print(f\"{'Student (Standard)':<25} {standard_student_accuracy:<10.2f} {standard_student_size:<12.2f} {standard_student_time*1000:<15.2f} {student_params:,}\")\n",
    "    print(f\"{'Student (Distilled)':<25} {distilled_accuracy:<10.2f} {distilled_size:<12.2f} {distilled_time*1000:<15.2f} {student_params:,}\")\n",
    "    print(f\"{'Student (Fine-tuned)':<25} {fine_tuned_accuracy:<10.2f} {distilled_size:<12.2f} {fine_tuned_time*1000:<15.2f} {student_params:,}\")\n",
    "\n",
    "    # Visualization\n",
    "    models = ['Teacher', 'Student\\nStandard', 'Student\\nDistilled', 'Student\\nFine-tuned']\n",
    "    accuracies = [teacher_accuracy, standard_student_accuracy, distilled_accuracy, fine_tuned_accuracy]\n",
    "    params = [teacher_params, student_params, student_params, student_params]\n",
    "    inference_times = [teacher_inference_time*1000, standard_student_time*1000,\n",
    "                       distilled_time*1000, fine_tuned_time*1000]\n",
    "\n",
    "    # Create bar charts\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Accuracy comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red'])\n",
    "    plt.title('Model Accuracy (%)')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Parameter comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(models, params, color=['blue', 'orange', 'green', 'red'])\n",
    "    plt.title('Model Parameters')\n",
    "    plt.ylabel('Parameters')\n",
    "\n",
    "    # Inference time comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(models, inference_times, color=['blue', 'orange', 'green', 'red'])\n",
    "    plt.title('Inference Time (ms)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "\n",
    "    # Size comparison\n",
    "    sizes = [teacher_size, standard_student_size, distilled_size, distilled_size]\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(models, sizes, color=['blue', 'orange', 'green', 'red'])\n",
    "    plt.title('Model Size (MB)')\n",
    "    plt.ylabel('Size (MB)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('knowledge_distillation_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix('Teacher', 'teacher_preds.npz')\n",
    "    plot_confusion_matrix('Standard_Student', 'standard_student_preds.npz')\n",
    "    plot_confusion_matrix('Distilled_Student', 'distilled_student_preds.npz')\n",
    "    plot_confusion_matrix('Fine_Tuned_Student', 'fine_tuned_student_preds.npz')\n",
    "\n",
    "    print(\"\\nVisualization saved as 'knowledge_distillation_comparison.png'\")\n",
    "    print(\"Confusion matrices saved for each model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a5bc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model has 688,138 parameters\n",
      "Loading pre-trained teacher model...\n",
      "\n",
      "--- Teacher Model Metrics ---\n",
      "Accuracy: 98.97%\n",
      "Parameters: 688,138\n",
      "Inference Time: 118.97 ms per batch\n",
      "Model Size: 2.63 MB\n",
      "\n",
      "Student model has 206,922 parameters\n",
      "Parameter reduction: 69.9%\n",
      "Loading pre-trained standard student model...\n",
      "\n",
      "--- Standard Student Model Metrics ---\n",
      "Accuracy: 98.77%\n",
      "Parameters: 206,922\n",
      "Inference Time: 112.56 ms per batch\n",
      "Model Size: 0.79 MB\n",
      "Loading pre-trained distilled student model...\n",
      "\n",
      "--- Distilled Student Model Metrics ---\n",
      "Accuracy: 98.65%\n",
      "Parameters: 206,922\n",
      "Inference Time: 111.10 ms per batch\n",
      "Model Size: 0.79 MB\n",
      "Loading pre-trained fine-tuned student model...\n",
      "\n",
      "--- Fine-tuned Student Model Metrics ---\n",
      "Accuracy: 99.12%\n",
      "Accuracy Improvement from Distillation: -0.12%\n",
      "Accuracy Improvement from Fine-tuning: 0.47%\n",
      "Inference Time: 112.09 ms per batch\n",
      "\n",
      "==================================================\n",
      "KNOWLEDGE DISTILLATION SUMMARY\n",
      "==================================================\n",
      "Model                     Accuracy   Size (MB)    Inference (ms)  Parameters  \n",
      "---------------------------------------------------------------------------\n",
      "Teacher                   98.97      2.63         118.97          688,138\n",
      "Student (Standard)        98.77      0.79         112.56          206,922\n",
      "Student (Distilled)       98.65      0.79         111.10          206,922\n",
      "Student (Fine-tuned)      99.12      0.79         112.09          206,922\n",
      "\n",
      "Visualization saved as 'knowledge_distillation_comparison.png'\n",
      "Confusion matrices saved for each model.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
