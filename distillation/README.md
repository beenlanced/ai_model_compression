## Distillation

This section of the project examines **Knowledge Distillation** of Neural Network models through a worked out example notebook. The notebook and materials are part of the LinkedIn Learning Course: [Ai Model Compression Techniques: Building Cheaper, Faster, and Greener AI:](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai), the Knowledge Distillation section.

---

### What is Knowledge Distillation?

Knowledge distillation is used to compress a complex and large neural network into a smaller and simpler one, while still retaining the accuracy and performance of the resultant model. This process involves training a smaller neural network to mimic the behavior of a larger and more complex "teacher" network by learning from its predictions or internal representations.

The goal of knowledge distillation is to reduce the memory footprint and computational requirements of a model without significantly sacrificing its performance. [[What is Knowledge Distillation? A Deep Dive.](https://blog.roboflow.com/what-is-knowledge-distillation/)]

Knowledge Distillation is known to follow a "Teacher-Student" architecture.

<p>
  <img alt="Teacher-Student Architecture" src="teacher_student.png" width="450" height="190"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

The process takes advantage of **Dark Knowledge**, the probability distribution relatiohnships between labels of a classification task.

<p>
  <img alt="Dark Knowledge" src="dark_knowledge.png" width="450" height="190"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

And, the knowledge transfer part of knowledge distillation uses **Soft Targets**.

#### How Does Knowledge Distillation Work? [taken from: [What is Knowledge Distillation? A Deep Dive.](https://blog.roboflow.com/what-is-knowledge-distillation/)]

Knowledge distillation involves two main steps: training the teacher network and training the student network.

During the first step, a large and complex neural network, or the teacher network, is trained on a dataset using a standard training procedure. Once the teacher network has been trained, it is used to generate "soft" labels for the training data, which are probability distributions over the classes instead of binary labels (i.e., hard labels). These soft labels are more informative than hard labels (e.g., the specific labels usually hot encoded by a learning model) and capture the uncertainty and ambiguity in the predictions of the teacher network.

In the second step, a smaller neural network, or the student network, is trained on the same dataset using the soft labels generated by the teacher network. The student network is trained to minimize the difference between its own predictions and the soft labels generated by the teacher network.

The intuition behind this approach is that the soft labels contain more information about the input data and the teacher network's predictions than the hard labels. Therefore, the student network can learn to capture this additional information and generalize better to new examples.

As shown below, a small “student” model learns to mimic a large “teacher” model and leverage the knowledge of the teacher to obtain similar or higher accuracy.

<p>
  <img alt="The teacher-student framework for knowledge distillation." src="framework.png" width="400" height="200"/>
</p>

[img source](https://arxiv.org/abs/2006.05525?ref=blog.roboflow.com) - The teacher-student framework for knowledge distillation

### Temperature Scaling In the Model Code - Helps Control Soft Target Distribution

<p>
  <img alt="Temperature Scaling" src="temperature_scaling.png" width="450" height="190"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

### Distillation Loss Helps to Evaluate Distillation Process

<p>
  <img alt="Distillation Loss Implementation" src="distillation_loss.png" width="450" height="200"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

### Feature Distillation Helps to Evaluate the Distillation Process as Well

<p>
  <img alt="Feature Distillation" src="feature_distillation.png" width="450" height="200"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

---

### Combine Distillation and Fine-Tuning to Get a Better Optimal Model with Improved Accuracy

Fine-tuning uses a lower learning rate to preserve knowledge by making gentle updates to weights.

Some best practices for Fine-Tuning:

- Use a learning rate 5-10 times smaller than the initial training

- Keep training epochs small (one to five typically sufficient)

- Monitor validation performance to prevent overfitting

---

### Summary of Results from Notebook

```
==================================================
KNOWLEDGE DISTILLATION SUMMARY
==================================================
Model                     Accuracy   Size (MB)    Inference (ms)  Parameters
---------------------------------------------------------------------------
Teacher                   98.97      2.63         118.97          688,138
Student (Standard)        98.77      0.79         112.56          206,922
Student (Distilled)       98.65      0.79         111.10          206,922
Student (Fine-tuned)      99.12      0.79         112.09          206,922

```

<p>
  <img alt="Knowledge Distilation Output Comparisons" src="knowledge_distillation_comparison.png" width="500" height="230"/>
</p>

The key differences between the Student (Standard) and Distilled Student Training:

<p>
  <img alt="Standard Student vs Distlled Student" src="stand_vs_distilled_student.png" width="400" height="200"/>
</p>

[img source: knowledge distillation](https://www.linkedin.com/learning/ai-model-compression-techniques-building-cheaper-faster-and-greener-ai)

## References

[Shrinking the Giants: How knowledge distillation is Changing the Landscape of Deep Learning Models](https://medium.com/@zone24x7_inc/shrinking-the-giants-how-knowledge-distillation-is-changing-the-landscape-of-deep-learning-models-83dffde577ec)

[What is Knowledge Distillation? A Deep Dive.](https://blog.roboflow.com/what-is-knowledge-distillation/)
