{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fa09fc",
   "metadata": {},
   "source": [
    "# Implementation in PyTorch\n",
    "\n",
    "### Static Quantization Example\n",
    "\n",
    "This is a simple Pytorch model with static quantization using FBGEMM (Facebook GEneral Matrix Multiplication). I have to define a Quantization-Ready Model.\n",
    "\n",
    "The model needs **QuantStub** and **DeQuantStub** layers to mark the points where tensors are quantized and dequantized.\n",
    "\n",
    "The **QuantStub** and **DeQuantStub** are crucial for defining the quantization boundaries, and the calibration step is essential for collecting the necessary statistics for accurate quantization.\n",
    "\n",
    "#### FBGEMM (Facebook GEneral Matrix Multiplication) \n",
    "\n",
    "It is a high-performance, low-precision library used as a backend for quantized operators on x86 machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af06d6a",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4361244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae6609",
   "metadata": {},
   "source": [
    "## Define a simple neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6253fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc190c31",
   "metadata": {},
   "source": [
    "## Instantiate and prepare the model for quantization\n",
    "\n",
    "#### `torch.quantization.get_default_qconfig('fbgemm')` \n",
    "\n",
    "Returns the default quantization configuration **(QConfig)** for the **FBGEMM** backend in PyTorch.\n",
    "\n",
    "This default QConfig is suitable for **post-training** static quantization and quantization-aware training when targeting x86 CPUs. It provides a common and effective configuration for achieving performance benefits with reduced precision while maintaining model accuracy.\n",
    "\n",
    "\n",
    "#### FBGEMM (Facebook GEneral Matrix Multiplication) \n",
    "\n",
    "It is a high-performance, low-precision library used as a backend for quantized operators on x86 machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f9b7b",
   "metadata": {},
   "source": [
    "## Prepare the Model for Quantization.\n",
    "Set the backend to \"fbgemm\" and get the default quantization configuration. Then, prepare the model, which inserts observers to collect statistics for quantization.\n",
    "\n",
    "\n",
    "### model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "Returns the default quantization configuration **(QConfig)** for the **FBGEMM** backend in PyTorch.\n",
    "\n",
    "This default QConfig is suitable for **post-training** static quantization and quantization-aware training when targeting x86 CPUs. It provides a common and effective configuration for achieving performance benefits with reduced precision while maintaining model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20e6bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = SimpleModel()\n",
    "model_fp32.eval() # Set model to evaluation mode for quantization\n",
    "\n",
    "# Specify FBGEMM backend for quantization\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "# This inserts observers to record activation statistics\n",
    "model_prepared = torch.quantization.prepare(model_fp32, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336525d0",
   "metadata": {},
   "source": [
    "## Calibrate the Model.\n",
    "\n",
    "Run the prepared model on a representative dataset. The observers collect statistics (min/max values) of activations, which are used to determine quantization `scale factors` and `zero-points`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e4c787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for calibration\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# Calibrate the model by running it with dummy data\n",
    "with torch.inference_mode():\n",
    "    model_prepared(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0cb58",
   "metadata": {},
   "source": [
    "## Convert to Quantized Model.\n",
    "Convert the prepared model to its quantized version using the collected statistics. This replaces floating-point operations with their quantized integer equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b77dad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prepared model to a quantized model\n",
    "model_quantized = torch.quantization.convert(model_prepared, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f5b0f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (quant): Quantize(scale=tensor([0.0224]), zero_point=tensor([75]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=10, out_features=20, scale=0.020005200058221817, zero_point=71, qscheme=torch.per_channel_affine)\n",
       "  (relu): ReLU()\n",
       "  (linear2): QuantizedLinear(in_features=20, out_features=5, scale=0.003096354193985462, zero_point=122, qscheme=torch.per_channel_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc498bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model-compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
